# -*- coding: utf-8 -*-
"""Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m7wKIGkgyvTwsAml3nZN9gnnj57b3sqC

# Modeling

## Overview
This notebook demonstrates the steps involved in training an image classification model using convolutional neural networks (CNNs) on Google Colab. We will go through data loading, preprocessing, model training, and evaluation.

### Steps Covered:
- Part 1: Loading and Normalizing Images
- Part 2: Defining and Training a Deep Learning Model with TensorFlow/Keras
- Part 3: (Optional) Distributed Training using TensorFlow’s Strategies
- Part 4: Saving Frequently Used Functions for Reusability
"""

# Import libraries
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from PIL import Image
import io
from skimage.feature import hog
from tqdm import tqdm
from keras.utils import to_categorical
import cv2
import json

# Define global variables
IMAGE_SIZE = (150, 150)  # Define your desired image size
base_path = '/content/drive/MyDrive/Colab Notebooks/Computer Vision Toolbox/data/binaryclass/cats_dogs'
datasets = [os.path.join(base_path, 'train'), os.path.join(base_path, 'test')]
class_names = ['cats', 'dogs']  # Example class names
class_names_label = {name: i for i, name in enumerate(class_names)}

# Function to load images and labels for binary classification
def load_data(base_path, image_size=(150, 150)):
    """
    Load images from train and test directories, resize them, and return as training and testing datasets.
    This version is adapted for binary classification with labels in {0, 1}.

    Args:
        base_path (str): Path to the base directory containing 'train' and 'test' directories.
        image_size (tuple): Desired image size as (width, height).

    Returns:
        tuple: Tuple containing training and testing datasets:
               (images_train, labels_train, images_test, labels_test).
    """
    datasets = {'train': [], 'test': []}
    for phase in ['train', 'test']:
        data_path = os.path.join(base_path, phase)
        images = []
        labels = []
        print("Loading {} data...".format(phase))

        for folder in os.listdir(data_path):
            class_label = 1 if folder.lower() == 'dogs' else 0  # Assign 1 to 'dogs', 0 to 'cats'
            folder_path = os.path.join(data_path, folder)

            for file in tqdm(os.listdir(folder_path)):
                img_path = os.path.join(folder_path, file)
                image = cv2.imread(img_path)

                if image is not None:
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                    image = cv2.resize(image, image_size)
                    image = image.astype(np.float32) / 255.0  # Normalize the images to [0, 1]

                    images.append(image)
                    labels.append(class_label)

        images = np.array(images, dtype='float32')
        labels = np.array(labels, dtype='int32')

        datasets[phase].append(images)
        datasets[phase].append(labels)

    return datasets['train'][0], datasets['train'][1], datasets['test'][0], datasets['test'][1]

# Load the data
images_train, labels_train, images_test, labels_test = load_data(base_path)

def create_model():
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
        layers.MaxPooling2D((2, 2)),
        # Add more layers as needed
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Create and train model
model = create_model()
history = model.fit(images_train, labels_train, epochs=10, validation_data=(images_test, labels_test))

# Print the keys available in the history dictionary
print(history.history.keys())

# Access loss and accuracy for each epoch
train_loss = history.history['loss']
train_accuracy = history.history['accuracy']
val_loss = history.history['val_loss']
val_accuracy = history.history['val_accuracy']

# You can print these values or plot them to visualize the trend
print("Training Loss: ", train_loss)
print("Training Accuracy: ", train_accuracy)
print("Validation Loss: ", val_loss)
print("Validation Accuracy: ", val_accuracy)

# Plotting the metrics
# Plot training & validation accuracy values
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

plt.show()

# Konvertujte history.history dictionary u JSON
history_json = json.dumps(history.history)

# Definišite putanju za čuvanje
path = "/content/drive/MyDrive/Colab Notebooks/Computer Vision Toolbox/history1.json"

# Sačuvajte JSON podatke u fajl
with open(path, "w") as f:
    f.write(history_json)

